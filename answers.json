"{"values": [{"recordId": 0, "data": {"data_points": ["Create table, define schema explicitly with SQL DDL\\n\\nCREATE TABLE [dbName.] tableName (\\n    id INT [NOT NULL],\\n    name STRING,\\n    date DATE,\\n    int_rate FLOAT)\\nUSING DELTA\\n[PARTITIONED BY (time, date)] -- optional\\n\\nCopy new data into Delta Lake table (with idempotent retries)\\n\\nCOPY INTO [dbName.] targetTable\\n  FROM (SELECT * FROM \\"/path/to/table\\")\\n  FILEFORMAT = DELTA  -- or CSV, Parquet, ORC, JSON, etc.\\n\\nDELTA LAKE DDL/DML:  UPDATE, DELETE, INSERT, ALTER TABLE\\n\\nTIME TRAVEL (CONTINUED)\\n\\nUpdate rows that match a predicate condition\\n\\nRollback a table to an earlier version\\n\\nUPDATE tableName SET event = 'click' WHERE event = 'clk'\\n\\nDelete rows that match a predicate condition\\n\\nDELETE FROM tableName WHERE \\"date < '2017-01-01\\"\\n\\nInsert values directly into table\\nINSERT INTO TABLE tableName VALUES (\\n    (8003, \\"Kim Jones\\", \\"2020-12-18\\", 3.875),\\n    (8004, \\"Tim Jones\\", \\"2020-12-20\\", 3.750) \\n);   \\n-- Insert using SELECT statement\\nINSERT INTO tableName SELECT * FROM sourceTable\\n-- Atomically replace all data in table with new values\\nINSERT OVERWRITE loan_by_state_delta VALUES (...)\\n\\nUpsert (update + insert) using MERGE\\n\\nMERGE INTO target\\nUSING updates\\nON target.Id = updates.Id\\nWHEN MATCHED AND target.delete_flag = \\"true\\" THEN\\n  DELETE\\nWHEN MATCHED THEN\\n  UPDATE SET *  -- star notation means all columns\\nWHEN NOT MATCHED THEN\\n  INSERT (date, Id, data) -- or, use INSERT *\\n  VALUES (date, Id, data)\\n\\nInsert with Deduplication using MERGE", "WITH SPARK SQL\\n\\nDelta Lake is an open source storage layer that brings ACID\\ntransactions to Apache Spark\u2122 and big data workloads.\\n\\ndelta.io | Documentation | GitHub | Delta Lake on Databricks\\n\\nCREATE AND QUERY DELTA TABLES\\n\\nCreate and use managed database\\n\\n-- Managed database is saved in the Hive metastore.\\nDefault database is named \\"default\\".\\nDROP DATABASE IF EXISTS dbName;\\nCREATE DATABASE dbName;\\nUSE dbName  -- This command avoids having to specify\\ndbName.tableName every time instead of just tableName.\\n\\nQuery Delta Lake table by table name (preferred)\\n\\n/* You can refer to Delta Tables by table name, or by\\npath. Table name is the preferred way, since named tables\\nare managed in the Hive Metastore (i.e., when you DROP a\\nnamed table, the data is dropped also \u2014 not the case for\\npath-based tables.) */\\nSELECT * FROM [dbName.] tableName\\n\\nQuery Delta Lake table by path\\n\\nSELECT * FROM delta.`path/to/delta_table` -- note\\nbackticks\\n\\nConvert Parquet table to Delta Lake format in place\\n\\n-- by table name\\nCONVERT TO DELTA [dbName.]tableName\\n[PARTITIONED BY (col_name1 col_type1, col_name2\\ncol_type2)]\\n\\n-- path-based tables\\nCONVERT TO DELTA parquet.`/path/to/table` -- note backticks\\n[PARTITIONED BY (col_name1 col_type1, col_name2 col_type2)]\\n\\nCreate Delta Lake table as SELECT * with no upfront \\nschema definition\\n\\nCREATE TABLE [dbName.] tableName\\nUSING DELTA\\nAS SELECT * FROM tableName | parquet.`path/to/data`\\n[LOCATION `/path/to/table`]  \\n-- using location = unmanaged table", "-- Deep clones copy data from source, shallow clones don't.\\nCREATE TABLE [dbName.] targetName\\n[SHALLOW | DEEP] CLONE sourceName [VERSION AS OF 0]\\n[LOCATION \\"path/to/table\\"]  \\n-- specify location only for path-based tables\\n\\nInteroperability with Python / DataFrames\\n-- Read name-based table from Hive metastore into DataFrame\\ndf = spark.table(\\"tableName\\")\\n-- Read path-based table into DataFrame\\ndf = spark.read.format(\\"delta\\").load(\\"/path/to/delta_table\\")\\nRun SQL queries from Python\\nspark.sql(\\"SELECT * FROM tableName\\")\\nspark.sql(\\"SELECT * FROM delta.`/path/to/delta_table`\\")\\nModify data retention settings for Delta Lake table\\n\\n-- logRetentionDuration -> how long transaction log history\\nis kept, deletedFileRetentionDuration -> how long ago a file\\nmust have been deleted before being a candidate for VACCUM.\\nALTER TABLE tableName\\nSET TBLPROPERTIES(\\n    delta.logRetentionDuration = \\"interval 30 days\\",\\n    delta.deletedFileRetentionDuration = \\"interval 7 days\\"\\n);\\nSHOW TBLPROPERTIES tableName;\\n\\nPERFORMANCE OPTIMIZATIONS\\n\\nCompact data files with Optimize and Z-Order\\n\\n*Databricks Delta Lake feature\\nOPTIMIZE tableName\\n[ZORDER BY (colNameA, colNameB)]\\n\\nAuto-optimize tables\\n\\n*Databricks Delta Lake feature\\nALTER TABLE [table_name | delta.`path/to/delta_table`]\\nSET TBLPROPERTIES (delta.autoOptimize.optimizeWrite = true)\\n\\nCache frequently queried data in Delta Cache\\n*Databricks Delta Lake feature\\nCACHE SELECT * FROM tableName\\n-- or:\\nCACHE SELECT colA, colB FROM tableName WHERE colNameA > 0"], "answer": " UPDATE tableName SET event = 'click' WHERE event = 'clk'\\n     \\n     <<What is the syntax for deleting data from a Delta Lake table?>> \\n     <<What is the syntax for upserting data in a Delta Lake table?>> \\n     <<What is the syntax for creating a Delta Lake table as SELECT * with no upfront schema definition?>>", "thoughts": "Searched for:<br> \\"Delta Lake\\" syntax update data table<br><br>Prompt:<br><|im_start|>system<br>    Be brief in your answers.<br>    Answer ONLY with the facts listed in the list of sources below. If there isn't enough information below, say you don't know. Do not generate answers that don't use the sources below. If asking a clarifying question to the user would help, ask the question.<br>    Each source has a name followed by colon and the actual information, always include the source name for each fact you use in the response. Use square brackets to reference the source, e.g. [info1.txt]. Don't combine sources, list each source separately, e.g. [info1.txt][info2.pdf].<br>    Generate three very brief follow-up questions that the user would likely ask next.<br>    Use double angle brackets to reference the questions, e.g. <<Is there a more details on that?>>.<br>    Try not to repeat questions that have already been asked.<br>    Only generate questions and do not generate any text before or after the questions, such as 'Next Questions'<br>    Sources:<br>    ['Create table, define schema explicitly with SQL DDL\\\\n\\\\nCREATE TABLE [dbName.] tableName (\\\\n    id INT [NOT NULL],\\\\n    name STRING,\\\\n    date DATE,\\\\n    int_rate FLOAT)\\\\nUSING DELTA\\\\n[PARTITIONED BY (time, date)] -- optional\\\\n\\\\nCopy new data into Delta Lake table (with idempotent retries)\\\\n\\\\nCOPY INTO [dbName.] targetTable\\\\n  FROM (SELECT * FROM \\"/path/to/table\\")\\\\n  FILEFORMAT = DELTA  -- or CSV, Parquet, ORC, JSON, etc.\\\\n\\\\nDELTA LAKE DDL/DML:  UPDATE, DELETE, INSERT, ALTER TABLE\\\\n\\\\nTIME TRAVEL (CONTINUED)\\\\n\\\\nUpdate rows that match a predicate condition\\\\n\\\\nRollback a table to an earlier version\\\\n\\\\nUPDATE tableName SET event = \\\\'click\\\\' WHERE event = \\\\'clk\\\\'\\\\n\\\\nDelete rows that match a predicate condition\\\\n\\\\nDELETE FROM tableName WHERE \\"date < \\\\'2017-01-01\\"\\\\n\\\\nInsert values directly into table\\\\nINSERT INTO TABLE tableName VALUES (\\\\n    (8003, \\"Kim Jones\\", \\"2020-12-18\\", 3.875),\\\\n    (8004, \\"Tim Jones\\", \\"2020-12-20\\", 3.750) \\\\n);   \\\\n-- Insert using SELECT statement\\\\nINSERT INTO tableName SELECT * FROM sourceTable\\\\n-- Atomically replace all data in table with new values\\\\nINSERT OVERWRITE loan_by_state_delta VALUES (...)\\\\n\\\\nUpsert (update + insert) using MERGE\\\\n\\\\nMERGE INTO target\\\\nUSING updates\\\\nON target.Id = updates.Id\\\\nWHEN MATCHED AND target.delete_flag = \\"true\\" THEN\\\\n  DELETE\\\\nWHEN MATCHED THEN\\\\n  UPDATE SET *  -- star notation means all columns\\\\nWHEN NOT MATCHED THEN\\\\n  INSERT (date, Id, data) -- or, use INSERT *\\\\n  VALUES (date, Id, data)\\\\n\\\\nInsert with Deduplication using MERGE', 'WITH SPARK SQL\\\\n\\\\nDelta Lake is an open source storage layer that brings ACID\\\\ntransactions to Apache Spark\u2122 and big data workloads.\\\\n\\\\ndelta.io | Documentation | GitHub | Delta Lake on Databricks\\\\n\\\\nCREATE AND QUERY DELTA TABLES\\\\n\\\\nCreate and use managed database\\\\n\\\\n-- Managed database is saved in the Hive metastore.\\\\nDefault database is named \\"default\\".\\\\nDROP DATABASE IF EXISTS dbName;\\\\nCREATE DATABASE dbName;\\\\nUSE dbName  -- This command avoids having to specify\\\\ndbName.tableName every time instead of just tableName.\\\\n\\\\nQuery Delta Lake table by table name (preferred)\\\\n\\\\n/* You can refer to Delta Tables by table name, or by\\\\npath. Table name is the preferred way, since named tables\\\\nare managed in the Hive Metastore (i.e., when you DROP a\\\\nnamed table, the data is dropped also \u2014 not the case for\\\\npath-based tables.) */\\\\nSELECT * FROM [dbName.] tableName\\\\n\\\\nQuery Delta Lake table by path\\\\n\\\\nSELECT * FROM delta.`path/to/delta_table` -- note\\\\nbackticks\\\\n\\\\nConvert Parquet table to Delta Lake format in place\\\\n\\\\n-- by table name\\\\nCONVERT TO DELTA [dbName.]tableName\\\\n[PARTITIONED BY (col_name1 col_type1, col_name2\\\\ncol_type2)]\\\\n\\\\n-- path-based tables\\\\nCONVERT TO DELTA parquet.`/path/to/table` -- note backticks\\\\n[PARTITIONED BY (col_name1 col_type1, col_name2 col_type2)]\\\\n\\\\nCreate Delta Lake table as SELECT * with no upfront \\\\nschema definition\\\\n\\\\nCREATE TABLE [dbName.] tableName\\\\nUSING DELTA\\\\nAS SELECT * FROM tableName | parquet.`path/to/data`\\\\n[LOCATION `/path/to/table`]  \\\\n-- using location = unmanaged table', '-- Deep clones copy data from source, shallow clones don\\\\'t.\\\\nCREATE TABLE [dbName.] targetName\\\\n[SHALLOW | DEEP] CLONE sourceName [VERSION AS OF 0]\\\\n[LOCATION \\"path/to/table\\"]  \\\\n-- specify location only for path-based tables\\\\n\\\\nInteroperability with Python / DataFrames\\\\n-- Read name-based table from Hive metastore into DataFrame\\\\ndf = spark.table(\\"tableName\\")\\\\n-- Read path-based table into DataFrame\\\\ndf = spark.read.format(\\"delta\\").load(\\"/path/to/delta_table\\")\\\\nRun SQL queries from Python\\\\nspark.sql(\\"SELECT * FROM tableName\\")\\\\nspark.sql(\\"SELECT * FROM delta.`/path/to/delta_table`\\")\\\\nModify data retention settings for Delta Lake table\\\\n\\\\n-- logRetentionDuration -> how long transaction log history\\\\nis kept, deletedFileRetentionDuration -> how long ago a file\\\\nmust have been deleted before being a candidate for VACCUM.\\\\nALTER TABLE tableName\\\\nSET TBLPROPERTIES(\\\\n    delta.logRetentionDuration = \\"interval 30 days\\",\\\\n    delta.deletedFileRetentionDuration = \\"interval 7 days\\"\\\\n);\\\\nSHOW TBLPROPERTIES tableName;\\\\n\\\\nPERFORMANCE OPTIMIZATIONS\\\\n\\\\nCompact data files with Optimize and Z-Order\\\\n\\\\n*Databricks Delta Lake feature\\\\nOPTIMIZE tableName\\\\n[ZORDER BY (colNameA, colNameB)]\\\\n\\\\nAuto-optimize tables\\\\n\\\\n*Databricks Delta Lake feature\\\\nALTER TABLE [table_name | delta.`path/to/delta_table`]\\\\nSET TBLPROPERTIES (delta.autoOptimize.optimizeWrite = true)\\\\n\\\\nCache frequently queried data in Delta Cache\\\\n*Databricks Delta Lake feature\\\\nCACHE SELECT * FROM tableName\\\\n-- or:\\\\nCACHE SELECT colA, colB FROM tableName WHERE colNameA > 0']<br>    <|im_end|><br>    <|im_start|>user<br> What is the syntax for converting a Parquet table to a Delta Lake format?<br><|im_end|><br><|im_start|>assistant<br> CONVERT TO DELTA [dbName.]tableName<br>     [PARTITIONED BY (col_name1 col_type1, col_name2 col_type2)]<br>     <br>     CONVERT TO DELTA parquet.`/path/to/table` -- note backticks<br>     [PARTITIONED BY (col_name1 col_type1, col_name2 col_type2)] <br>     <br>     [DeltaTable.convertToDelta(spark, \\"parquet.`/path/to/parquet_table`\\")]<br>     <br>     [DeltaTable.convertToDelta(spark, \\"parquet.`/path/to/parquet_table`\\", \\"part int\\")] <br>     <br>     <<What is the difference between converting a Parquet table to Delta Lake format by table name and by path?>> <br>     <<Can you convert a partitioned Parquet table to Delta Lake format?>> <br>     <<What is the syntax for converting a Delta Lake table to a Parquet table?>><|im_end|><br><|im_start|>user<br>What is the difference between converting a Parquet table to Delta Lake format by table name and by path?<br><|im_end|><br><|im_start|>assistant<br> Converting a Parquet table to Delta Lake format by table name is preferred since named tables are managed in the Hive Metastore. When you DROP a named table, the data is dropped also, which is not the case for path-based tables. <br>     <br>     Converting a Parquet table to Delta Lake format by path requires using backticks and specifying the path to the table. <br>     <br>     <<What is the difference between a named table and a path-based table in Delta Lake?>> <br>     <<What is the syntax for querying a Delta Lake table by table name?>> <br>     <<Can you convert a non-partitioned Parquet table to Delta Lake format?>><|im_end|><br><|im_start|>user<br>What is the syntax for querying a Delta Lake table by table name?<br><|im_end|><br><|im_start|>assistant<br> SELECT * FROM [dbName.] tableName<br>     <br>     <<What is the syntax for querying a Delta Lake table by path?>> <br>     <<What is the difference between a named table and a path-based table in Delta Lake?>> <br>     <<What is the syntax for creating a Delta Lake table with an explicit schema?>><|im_end|><br><|im_start|>user<br>What is the syntax for creating a Delta Lake table with an explicit schema?<br><|im_end|><br><|im_start|>assistant<br> CREATE TABLE [dbName.] tableName (<br>         id INT [NOT NULL],<br>         name STRING,<br>         date DATE,<br>         int_rate FLOAT<br>     )<br>     USING DELTA<br>     [PARTITIONED BY (time, date)] -- optional<br>     <br>     <<What are the data types supported by Delta Lake?>> <br>     <<What is the syntax for inserting data into a Delta Lake table?>> <br>     <<What is the syntax for updating data in a Delta Lake table?>><|im_end|><br><|im_start|>user<br>What is the syntax for inserting data into a Delta Lake table?<br><|im_end|><br><|im_start|>assistant<br> INSERT INTO TABLE tableName VALUES (<br>         (8003, \\"Kim Jones\\", \\"2020-12-18\\", 3.875),<br>         (8004, \\"Tim Jones\\", \\"2020-12-20\\", 3.750) <br>     );   <br>     <br>     INSERT INTO tableName SELECT * FROM sourceTable<br>     <br>     INSERT OVERWRITE loan_by_state_delta VALUES (...)<br><br>     <<What is the syntax for creating a Delta Lake table as SELECT * with no upfront schema definition?>> <br>     <<What is the syntax for updating data in a Delta Lake table?>> <br>     <<What is the syntax for deleting data from a Delta Lake table?>><|im_end|><br><|im_start|>user<br>What is the syntax for updating data in a Delta Lake table?<br><|im_end|><br><|im_start|>assistant<br><br><br>    "}}]}"
